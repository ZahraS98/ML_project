ML Project
Instructions
You should choose two datasets among the following ones. You should divide your group (hexanome) in two subgroups. Each subgroup works on one dataset. For each dataset, you should write a report in the form of an executable notebook either in R or Python (or even Julia). 

The report should follow these guidelines:

Explore the dataset: You can, for example, use the approach of chapter 10 ("SVD et analyse exploratoire d’un jeu de données"). However, you are free to experiment with other approaches aimed at exploratory data analysis.
Train different models: For example, you can consider the models introduced during the course: linear regression, ridge regression, kernel ridge regression, nyström approximation for kernel ridge regression. You should also consider at least one additional model, not introduced during the course, such as random forest, multi-layers perceptron, etc. For this additional model, you should explain what are its parameters and hyper-parameters. You should also explain how its hyper-parameters are related to the bias-variance tradeoff.
For a quick, accurate and practical introduction to ensemble models (such as random forest, gradient boosting, etc.) and deep neural network, you can use the following reference : https://www.sciencedirect.com/science/article/pii/S0370157319300766
For at least one of these models, you should try to understand in which situations the model commits the biggest errors. Can this information help you to build a better model?
You could also look at explanatory tools, such as the SHAP Python library (https://github.com/slundberg/shap) or an R version of this same game-theoretic approach (e.g., https://cran.r-project.org/web/packages/shapr/vignettes/understanding_shapr.html).
If the dataset is small enough for full kernel ridge regression, you could then use the R package KRLS (https://cran.r-project.org/web/packages/KRLS/index.html) which offers tools that makes the model intrinsically interpretable (see for example: https://web.stanford.edu/~jhain/Paper/JSS2015_RR.pdf).
You should compare the different models (e.g., by k-fold cross-validation), choose the best one and evaluate it on a test dataset never used before.
Your two groups work in parallel, each on a different dataset. However, you should share good practices, techniques you discover (for example, for dealing with missing values), and share as much as possible during your parallel analysis.


Proposed datasets
1. Fish Market Dataset: This dataset contains information about the weight and measurements of 159 fish from seven species. It has 7 attributes, including the weight of each fish. The task is to predict the weight based on the other features. You can find this dataset on Kaggle (https://www.kaggle.com/datasets/aungpyaeap/fish-market). It is relatively small (159 instances), clean and simple. It has only numerical features that are easy to measure and interpret. The target variable (weight) is also numerical and continuous.
2. House Sales in King County, USA: This dataset contains information about house sales in King County, Washington, USA. It has 21613 instances and 21 attributes, including the price of each house. The task is to predict the price based on the other features. You can find this dataset on Kaggle (https://www.kaggle.com/datasets/harlfoxem/housesalesprediction). This dataset is relatively large and clean. It has mostly numerical features that are easy to measure and interpret. The target variable (price) is also numerical and continuous.
3. Concrete Compressive Strength Dataset: This dataset contains information about the compressive strength of concrete samples. It has 1030 instances and 9 attributes, including the compressive strength of each sample. The task is to predict the compressive strength based on the other features. You can find this dataset on Kaggle (https://www.kaggle.com/datasets/elikplim/concrete-compressive-strength-data-set). This dataset is relatively small and clean. It has only numerical features that are easy to measure and interpret. The target variable (compressive strength) is also numerical and continuous.
4. Medical Cost Personal Dataset: This dataset contains information about the medical costs of 1338 individuals in the US. It has 7 attributes, including the charges of each individual. The task is to predict the charges based on the other features. You can find this dataset on Kaggle (https://www.kaggle.com/datasets/mirichoi0218/insurance). This dataset is relatively small and clean. It has mostly numerical features that are easy to measure and interpret. The target variable (charges) is also numerical and continuous. However, some challenges may arise from the skewed distribution of the target variable, which may require some transformation or normalization.
5. Diabetes Dataset: This dataset contains information about diabetes progression in 442 patients. It has 10 attributes, including the quantitative measure of disease progression one year after baseline. The task is to predict the disease progression based on the other features. You can find this dataset on Kaggle (https://www.kaggle.com/datasets/mathchi/diabetes-data-set). This dataset is relatively small, and clean. It has only numerical features that are easy to measure and interpret. The target variable (disease progression) is also numerical and continuous. However, some challenges may arise from the high correlation among some of the features, which may require some feature selection or dimensionality reduction.
6. Life Expectancy Dataset: This dataset contains information about the life expectancy at birth of 193 countries. It has 22 attributes, including the life expectancy of each country. The task is to predict the life expectancy based on the other features. You can find this dataset on Kaggle (https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who). This dataset is relatively large (2938 instances), noisy and complex. It has a mix of numerical and categorical features that may require some encoding and scaling. The target variable (life expectancy) is also numerical and continuous. Some challenges may arise from the missing values, outliers, multicollinearity, etc.
7. Boston Housing Dataset: This dataset contains information about housing prices in Boston, Massachusetts. It has 506 instances and 14 attributes, including the median value of owner-occupied homes. The task is to predict the median value based on the other features. You can find this dataset on Kaggle (https://www.kaggle.com/datasets/altavish/boston-housing-dataset). This dataset is relatively small, and clean. It has only numerical features that are easy to measure and interpret. The target variable (median value of owner-occupied homes) is also numerical and continuous. However, some challenges may arise from the non-linear relationship between some of the features and the target variable, which may require some feature engineering or transformation.
8. Auto MPG Dataset: This dataset contains information about fuel consumption and performance of various cars. It has 398 instances and 9 attributes, including the miles per gallon (MPG) of each car. The task is to predict the MPG based on the other features. You can find this dataset on Kaggle (https://www.kaggle.com/datasets/uciml/autompg-dataset). This dataset is relatively small, but noisy. Some challenges may arise from the missing values, outliers, multicollinearity, and non-linearity in the data.
9. Wine Quality Dataset: This dataset contains information about the quality of red and white wines. It has 6497 instances and 12 attributes, including the quality rating of each wine. The task is to predict the quality rating based on the other features. You can find this dataset on Kaggle (https://www.kaggle.com/datasets/yasserh/wine-quality-dataset). This dataset is relatively large, and noisy. It has only numerical features that may require some scaling and normalization. The target variable (quality) is also numerical but discrete and ordinal. Some challenges may arise from the imbalanced distribution of the target variable, which may require some resampling or weighting techniques.
10. Student Performance Dataset: This dataset contains information about the academic performance of 649 Portuguese students. It has 33 attributes, including the final grade of each student. The task is to predict the final grade based on the other features. You can find this dataset on Kaggle (https://www.kaggle.com/datasets/impapan/student-performance-data-set). This dataset is relatively small, but noisy. It has a mix of numerical and categorical features that may require some encoding and scaling. The target variable (final grade) is also numerical but discrete and ordinal. Some challenges may arise from the missing values, outliers, multicollinearity, non-linearity, and interaction effects in the data.


Deliverables
Your notebooks in their source format (.Rmd for R, .ipynb for Python) and as a PDF export.
At the beginning of a notebook, put clearly the names of the authors of the notebook and the name of your hexanome.
Send a zip archive with your notebooks.